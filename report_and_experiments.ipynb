{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2 - Text Analysis using Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "The problem selected was the first option: \"Problema 1 - Determinação de Valência em Manchetes de Jornais Brasileiros no 1° Semestre de 2017\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The datase used in this experiment is avaliable in [Manchetes de Jornais Brasileiros](https://github.com/pdpcosta/manchetesBrasildatabase)\n",
    "The dataset has 5 columns:\n",
    "* Day \n",
    "* Month \n",
    "* Year\n",
    "* Newspaper names\n",
    "* Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "The Classifier selected was [Bayes Naive](http://scikit-learn.org/stable/modules/naive_bayes.html). As my first implementation, I decided this one, because of the simplicity and documentation avaliable (easy to find documentation, tutorials and examples). \n",
    "Besides that, I found an interesting example using a portuguese train dataset labeled ('Positive', 'Neutral' and 'Negative'). The example is available in [Md Repo](https://github.com/minerandodados/mdrepo).\n",
    "\n",
    "The Bayes Naive Classifier uses Bayes Theorem to predict the probability that a given feature set belongs to a particular label. The implementation presented here uses the Extraction Feature **tf-idf**\n",
    "\n",
    "### Language and Libraries\n",
    "For running the experiments the following languages and libraries were selected:\n",
    "\n",
    "1. Environment: [Anaconda3 4.3.1](https://repo.continuum.io/archive/index.html)\n",
    "2. Programming Language: [Python 3.3](https://www.python.org/) \n",
    "3. Dataframe Library: [Panda 0.19.2](http://pandas.pydata.org/).\n",
    "4. Natural Language: [NLTK 3.2.4](http://www.nltk.org/)\n",
    "5. Machine Learning: [Scikit-learn](http://scikit-learn.org/stable/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The preprocessing steps were:\n",
    "1. Replace the character separator comma (Problems with quantity of columns) \n",
    "2. Clean special caracteres\n",
    "3. Tokenize the sentences\n",
    "4. Delete the stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Openning dataset and replace separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasetNews = open(\"manchetesBrasildatabase.csv\", mode=\"r\", encoding=\"utf-8\")\n",
    "datasetNewsPreprocessed = open(\"manchetesBrasildatabaseProcessada.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "for row in datasetNews:\n",
    "     \n",
    "    tupleNews = str(row)\n",
    "    \n",
    "    tupleNews = tupleNews.replace(\",\",\"|\")\n",
    "    \n",
    "    separatedRow = tupleNews.split(\"|\")\n",
    "\n",
    "    if len(separatedRow) > 5:\n",
    "        titleNews = \"\"\n",
    "        count = 4\n",
    "        while count < len(separatedRow):\n",
    "            titleNews += separatedRow[count]\n",
    "            if (count < len(separatedRow)-1):\n",
    "                titleNews += \", \"\n",
    "            count+=1\n",
    "        separatedRow[4] = titleNews\n",
    "        \n",
    "    tuplePreprocessed = separatedRow[0] + \" | \" + separatedRow[1] + \" | \" + separatedRow[2] \\\n",
    "                        + \" | \" + separatedRow[3] + \" | \" + separatedRow[4]\n",
    "    \n",
    "    datasetNewsPreprocessed.write(tuplePreprocessed)\n",
    "\n",
    "datasetNews.close()\n",
    "datasetNewsPreprocessed.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Openning the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetInFrame = pd.read_csv(\"manchetesBrasildatabaseProcessada.csv\",header=None, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasetInFrame.columns =['Dia', 'Mês', 'Ano', 'Jornal', 'Título']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasetInFrame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning special characteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [ str(sentence).lower().replace(\"'\",\"\").replace(\".\",\"\").replace(\",\",\"\") for sentence in datasetInFrame[\"Título\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#portuguese_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "\n",
    "sentencesWithTokens = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('portuguese')\n",
    "\n",
    "sentencesWithoutStopWords = [word for word in sentencesWithTokens if word not in stop_words]\n",
    "\n",
    "sentencesWithoutStopWords = []\n",
    "\n",
    "#Navegando no conjunto de dados\n",
    "for row in sentencesWithTokens:\n",
    "    sentence = \"\"\n",
    "    for word in row:\n",
    "        if not word in stop_words:\n",
    "            sentence = sentence + \" \"+ str(word)\n",
    "    sentencesWithoutStopWords.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentencesWithoutStopWords\n",
    "#nltk.corpus.mac_morpho.words()\n",
    "#tags = nltk.pos_tag(sentencesWithTokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.corpus.mac_morpho.words()\n",
    "\n",
    "#tsents = nltk.corpus.mac_morpho.tagged_sents()\n",
    "#train = tsents[100:]\n",
    "#test = tsents[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Using Bayes Naive\n",
    "Source: [Md Repo](https://github.com/minerandodados/mdrepo)\n",
    "The dataset used to train the classifier is composed by 9817 Tweets labeled with \"Positivo\", \"Neutro\" and \"Negativo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasetLabeled = pd.read_csv('Tweets_Mg.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetLabeled[datasetLabeled.Classificacao=='Neutro'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetLabeled[datasetLabeled.Classificacao=='Positivo'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetLabeled[datasetLabeled.Classificacao=='Negativo'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = datasetLabeled['Text'].values\n",
    "classes = datasetLabeled['Classificacao'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayse\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. [Multinomial Naive Bayse](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\")\n",
    "freq_tweets = vectorizer.fit_transform(tweets)\n",
    "model = MultinomialNB()\n",
    "model.fit(freq_tweets,classes)\n",
    "freq_tests = vectorizer.transform(sentences)\n",
    "prediction1 = model.predict(freq_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences Without Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\")\n",
    "freq_tweets = vectorizer.fit_transform(tweets)\n",
    "model = MultinomialNB()\n",
    "model.fit(freq_tweets,classes)\n",
    "freq_tests = vectorizer.transform(sentencesWithoutStopWords)\n",
    "prediction2 = model.predict(freq_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results  Sentences Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    print(\"'\" + sentences[i] + \"' - valence: \" + prediction1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasetInFrame['Valência'] = pd.Series(prediction1, index=datasetInFrame.index)\n",
    "#datasetInFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Sentences Without StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    print(\"'\" + sentencesWithoutStopWords[i] + \"' - valence: \" + prediction2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(prediction1 == prediction2)\n",
    "for i in range(500):\n",
    "    if (prediction1[i] != prediction2[i]) : \n",
    "        print(\"Description: Sentence and Sentence2 (WithoutStopWords)\")\n",
    "        print(\"Sentence1:\"  + sentences[i] + \": \"+ prediction1[i])\n",
    "        print(\"Sentence2:\"  + sentencesWithoutStopWords[i] + \": \"+ prediction2[i])\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Conclusion\n",
    "\n",
    "Evaluating the results, I don't agree with the most labels. The classifier labeled wrong even in sentences with words that represent some \"sentiment\":\n",
    "\n",
    "* \"cortes e déficit\" -> neutro\n",
    "* \"violência no rio: assassinatos sobem 41 37%\" -> neutro\n",
    "* \"jovens estudam com medo\" -> positivo\n",
    "\n",
    "\n",
    "Some aspects to consider:\n",
    "* The classifier doesn't consider the relationship between words, probably it is a restriction. \n",
    "* The example that I select has a train dataset doesn't reflect the same context (Tweeter vs Newspaper).\n",
    "* Comparing the results between Sentences and Sentences Without StopWords, some results are better, maybe apply in the train dataset can be an option. \n",
    "\n",
    "Futures improvements (in  my point of view):\n",
    "* Improve the preprocessing steps, there are some special caracteres that were not treated, and incomplete sentences;\n",
    "* Consider other corpus and extend them, for this implementation, the dataset test was not enough;\n",
    "* Explore and analyse better the train dataset;\n",
    "* Use the POS tagging;\n",
    "* Explore N-grams, to explore the relationhip among words in sentence;\n",
    "* Explore other classifiers (this [Nlpnet](http://nilc.icmc.usp.br/nlpnet/) looks interesting);\n",
    "* There are some interesting train labeled datasets in English, one option is translate and explore it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Python Text Processing with NLTK 2.0 Cookbook](https://www.amazon.com.br/Python-Text-Processing-Nltk-Cookbook/dp/1849513600)\n",
    "\n",
    "[Blog Chapagain](http://blog.chapagain.com.np/machine-learning-sentiment-analysis-text-classification-using-python-nltk/)\n",
    "\n",
    "[Blog iMasters](https://imasters.com.br/desenvolvimento/analise-de-sentimentos-aprenda-de-uma-vez-por-todas-como-funciona-utilizando-dados-do-twitter/?trace=1519021197&source=single)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
